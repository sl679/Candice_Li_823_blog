{
  
    
        "post0": {
            "title": "Ridge regression and Random forest regression",
            "content": "Import necessary packages . import warnings warnings.filterwarnings(&#39;ignore&#39;) from pandas import DatetimeIndex from pandas import Timestamp import pandas as pd from pandas import read_csv from pandas import DataFrame from pandas import concat import matplotlib.pyplot as plt import seaborn as sns import numpy as np from numpy import asarray import datetime as dt from datetime import timedelta import time from sklearn.linear_model import Ridge from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor from sklearn.metrics import mean_squared_error,r2_score, mean_absolute_error import plotly.express as px . Read in the dataset and sort by date . df = pd.read_csv(&quot;United_States_COVID-19_Cases_and_Deaths_by_State_over_Time.csv&quot;) df[&quot;submission_date&quot;] = pd.to_datetime(df[&quot;submission_date&quot;]) df = df.sort_values(by=&quot;submission_date&quot;) . Ridge regression . Filtering out the data in desired range . start_date = &#39;2020-12-14&#39; end_date = &#39;2021-11-19&#39; . mask = (df[&#39;submission_date&#39;] &gt;= start_date) &amp; (df[&#39;submission_date&#39;] &lt;= end_date) df = df.loc[mask] . Define few functions for further use. . def split_train_test(n, df): train = df.iloc[:int(df.shape[0]*n)] valid = df.iloc[int(df.shape[0]*n):] return train, valid . def plot_result(data,new_date_time_index, forecast, gcolor, gcase, gtitle): plt.plot(data,label=&quot;Actual &quot;+gcase,color=gcolor, linestyle=&#39;solid&#39;, linewidth = 1, marker=&#39;o&#39;, markerfacecolor=gcolor, markersize=1) plt.plot(new_date_time_index,forecast,label=&quot;Predicted &quot;+gcase,color=&#39;black&#39;, linestyle=&#39;solid&#39;, linewidth = 1, marker=&#39;*&#39;, markerfacecolor=&#39;black&#39;, markersize=1) . def new_forecast(prediction,new_prediction,new_date): forecast=np.concatenate((prediction,new_prediction)) new_ar = [] for single_timestamp in datewise_state.index: new_ar.append(pd.to_datetime(single_timestamp)) for single_timestamp in new_date: new_ar.append(pd.to_datetime(single_timestamp)) new_date_time_index = DatetimeIndex(new_ar, dtype=&#39;datetime64[ns]&#39;, name=&#39;Date&#39;, freq=None) return (forecast,new_date_time_index) . def eval_reg(y,y_p): print(&quot;Mean Absolute Error: &quot;,mean_absolute_error(y,y_p)) print(&quot;R2-Squared:&quot;, r2_score(y,y_p)) . def RidgeRegression_covid_19(aph,train,valid,data,case, datewise_state): model_scores=[] rf=Ridge(alpha=aph) rf.fit(np.array(train_ml[&quot;Days Since&quot;]).reshape(-1,1),np.array(train).reshape(-1,1)) prediction_valid_rf=rf.predict(np.array(valid_ml[&quot;Days Since&quot;]).reshape(-1,1)) model_scores.append(np.sqrt(mean_squared_error(valid,prediction_valid_rf))) print(&quot;RMSE for Ridge Regression Regression: &quot;,model_scores) prediction_rf=rf.predict(np.array(datewise_state[&quot;Days Since&quot;]).reshape(-1,1)) new_date=[] new_prediction_rf=[] eval_reg(valid,prediction_valid_rf) for i in range(1,8): new_date.append(datewise_state.index[-1]+timedelta(days=i)) new_prediction_rf.append(rf.predict(np.array(datewise_state[&quot;Days Since&quot;].max()+i).reshape(-1,1))[0]) forecast_rf, new_date_time_index =new_forecast(prediction_rf,new_prediction_rf,new_date) plot_result(data,new_date_time_index, forecast_rf,&#39;blue&#39;, case, &#39;Ridge Regression Regression Prediction&#39;) return (forecast_rf, new_date_time_index,model_scores) . Building ridge regression model for California . df_state_ca = df[df[&quot;state&quot;] ==&#39;CA&#39;] datewise_ca=df_state_ca.groupby([&quot;submission_date&quot;]).agg({&quot;tot_cases&quot;:&#39;sum&#39;,&quot;tot_death&quot;:&#39;sum&#39;}) . datewise_ca[&quot;Days Since&quot;] = datewise_ca.index-datewise_ca.index[0] datewise_ca[&quot;Days Since&quot;] = datewise_ca[&quot;Days Since&quot;].dt.days datewise_ca . tot_cases tot_death Days Since . submission_date . 2020-12-14 1585044 | 21088 | 0 | . 2020-12-15 1617370 | 21178 | 1 | . 2020-12-16 1671081 | 21410 | 2 | . 2020-12-17 1723362 | 21817 | 3 | . 2020-12-18 1764374 | 22080 | 4 | . ... ... | ... | ... | . 2021-11-15 4992875 | 72570 | 336 | . 2021-11-16 4997390 | 72590 | 337 | . 2021-11-17 5001302 | 72718 | 338 | . 2021-11-18 5009101 | 72847 | 339 | . 2021-11-19 5014428 | 73000 | 340 | . 341 rows × 3 columns . Split the data set into training and testing by 80:20 . train_ml_ca, valid_ml_ca = split_train_test(0.8, datewise_ca) . plt.scatter(df_state_case[&quot;submission_date&quot;], df_state_case[&quot;tot_cases&quot;]) plt.show() . forecast_r_ca, new_date_time_index_ca, model_score_r_ca = RidgeRegression_covid_19(20,train_ml_ca[&quot;tot_cases&quot;],valid_ml_ca[&quot;tot_cases&quot;],datewise_ca[&quot;tot_cases&quot;],&#39;Confirmed Cases&#39;,datewise_ca) . RMSE for Ridge Regression Regression: [132687.1061091103] Mean Absolute Error: 132390.1686492473 R2-Squared: 0.015833285852311363 . One week predictions for California . forecast_r_ca[-7:] . 4913282.596005402 . Average value for prediction for the next week in California . np.mean(forecast_r_ca[-7:]) . 4913282.596005402 . The predicted average total cases for California in the next 7 days after November 19th is 4913282.596. . Building model for Florida . df_state_fl = df[df[&quot;state&quot;] == &#39;FL&#39;] plt.scatter(df_state_fl[&quot;submission_date&quot;], df_state_fl[&quot;tot_cases&quot;]) plt.show() . datewise_fl = df_state_fl.groupby([&quot;submission_date&quot;]).agg({&quot;tot_cases&quot;:&#39;sum&#39;,&quot;tot_death&quot;:&#39;sum&#39;}) . tot_cases tot_death . submission_date . 2020-12-14 1122656 | 21409 | . 2020-12-15 1133968 | 21524 | . 2020-12-16 1146807 | 21630 | . 2020-12-17 1159721 | 21745 | . 2020-12-18 1171104 | 21860 | . ... ... | ... | . 2021-11-15 3672616 | 61074 | . 2021-11-16 3674329 | 61078 | . 2021-11-17 3676337 | 61080 | . 2021-11-18 3677967 | 61081 | . 2021-11-19 3679548 | 61081 | . 341 rows × 2 columns . datewise_fl[&quot;Days Since&quot;] = datewise_fl.index-datewise_fl.index[0] datewise_fl[&quot;Days Since&quot;] = datewise_fl[&quot;Days Since&quot;].dt.days datewise_fl train_ml_fl, valid_ml_fl = split_train_test(0.8, datewise_fl) . forecast_r_fl, new_date_time_index_fl, model_score_r_fl = RidgeRegression_covid_19(20,train_ml_fl[&quot;tot_cases&quot;],valid_ml_fl[&quot;tot_cases&quot;],datewise_fl[&quot;tot_cases&quot;],&#39;Confirmed Cases&#39;, datewise_fl) . RMSE for Ridge Regression Regression: [362609.5535802476] Mean Absolute Error: 355831.9658269108 R2-Squared: -31.073739984834212 . Prediction for the next week in Florida . forecast_r_fl[-7:] . array([[3470330.84849097], [3476729.63614643], [3483128.42380189], [3489527.21145735], [3495925.99911281], [3502324.78676827], [3508723.57442373]]) . Average value for prediction for the next week in Florida . np.mean(forecast_r_fl[-7:]) . 3489527.211457348 . The predicted average total cases for Florida in the next 7 days after November 19th is 3489527.211. . Building model for Texas . df_state_tx = df[df[&quot;state&quot;] == &#39;TX&#39;] plt.scatter(df_state_tx[&quot;submission_date&quot;], df_state_tx[&quot;tot_cases&quot;]) plt.show() . datewise_tx = datewise_tx.groupby([&quot;submission_date&quot;]).agg({&quot;tot_cases&quot;:&#39;sum&#39;,&quot;tot_death&quot;:&#39;sum&#39;}) . tot_cases tot_death . submission_date . 2020-12-14 1482141 | 27183 | . 2020-12-15 1500538 | 27402 | . 2020-12-16 1519340 | 27616 | . 2020-12-17 1539189 | 27851 | . 2020-12-18 1555981 | 28096 | . ... ... | ... | . 2021-11-15 4268082 | 71633 | . 2021-11-16 4272442 | 71722 | . 2021-11-17 4277364 | 71844 | . 2021-11-18 4282152 | 71982 | . 2021-11-19 4286622 | 72082 | . 341 rows × 2 columns . datewise_tx[&quot;Days Since&quot;] = datewise_tx.index-datewise_tx.index[0] datewise_tx[&quot;Days Since&quot;] = datewise_tx[&quot;Days Since&quot;].dt.days datewise_tx train_ml_tx, valid_ml_tx = split_train_test(0.8, datewise_tx) . One week predictions for Texas . forecast_r_tx, new_date_time_index_tx, model_score_r_tx = RidgeRegression_covid_19(20,train_ml_tx[&quot;tot_cases&quot;],valid_ml_tx[&quot;tot_cases&quot;],datewise_tx[&quot;tot_cases&quot;],&#39;Confirmed Cases&#39;, datewise_tx) . RMSE for Ridge Regression Regression: [360739.008769759] Mean Absolute Error: 358889.2838986177 R2-Squared: -6.605392141677496 . Average value for prediction for the next week in Texas . np.mean(forecast_r_tx[-7:]) . 3984442.8460686724 . The predicted average total cases for Texas in the next 7 days after November 19th is 3984442.846. . Random forest models . Define some functions for random forest for later use . def train_test_split(data, n_test): return data[:-n_test, :], data[-n_test:, :] . def series_to_supervised(data, n_in=1, n_out=1, dropnan=True): n_vars = 1 if type(data) is list else data.shape[1] df = DataFrame(data) cols = list() # input sequence (t-n, ... t-1) for i in range(n_in, 0, -1): cols.append(df.shift(i)) # forecast sequence (t, t+1, ... t+n) for i in range(0, n_out): cols.append(df.shift(-i)) # put it all together agg = concat(cols, axis=1) # drop rows with NaN values if dropnan: agg.dropna(inplace=True) return agg.values . def random_forest_forecast(train, testX): train = asarray(train) trainX, trainy = train[:, :-1], train[:, -1] model = RandomForestRegressor(n_estimators=1000) model.fit(trainX, trainy) yhat = model.predict([testX]) return yhat[0] . def walk_forward_validation(data, n_test): predictions = list() train, test = train_test_split(data, n_test) history = [x for x in train] for i in range(len(test)): testX, testy = test[i, :-1], test[i, -1] yhat = random_forest_forecast(history, testX) predictions.append(yhat) history.append(test[i]) print(&#39;&gt;expected=%.1f, predicted=%.1f&#39; % (testy, yhat)) error = mean_absolute_error(test[:, -1], predictions) print(predictions) return error, test[:, -1], predictions . The R^2 for the ridge regression are not very high. The value for FLorida and Texas are negative, indicating the model does not have a good fit. . Random forest model for California . series_ca = df_state_ca[[&#39;tot_cases&#39;]] values_ca = series_ca.values . data_ca = series_to_supervised(values_ca, n_in=6) # evaluate mae, y, yhat = walk_forward_validation(data_ca, 14) print(&#39;MAE: %.3f&#39; % mae) # plot expected vs predicted plt.plot(y, label=&#39;Expected&#39;) plt.plot(yhat, label=&#39;Predicted&#39;) plt.legend() plt.show() . &gt;expected=4937188.0, predicted=4932200.4 &gt;expected=4937188.0, predicted=4935532.8 &gt;expected=4956081.0, predicted=4936692.6 &gt;expected=4960026.0, predicted=4949281.5 &gt;expected=4963742.0, predicted=4955683.5 &gt;expected=4972469.0, predicted=4960634.1 &gt;expected=4972469.0, predicted=4967925.9 &gt;expected=4984465.0, predicted=4970874.9 &gt;expected=4984465.0, predicted=4979996.6 &gt;expected=4992875.0, predicted=4982649.1 &gt;expected=4997390.0, predicted=4989051.0 &gt;expected=5001302.0, predicted=4994089.6 &gt;expected=5009101.0, predicted=4998785.6 &gt;expected=5014428.0, predicted=5005544.6 [4932200.403, 4935532.799, 4936692.647, 4949281.463, 4955683.496, 4960634.143, 4967925.924, 4970874.919, 4979996.582, 4982649.143, 4989051.028, 4994089.569, 4998785.575, 5005544.616] MAE: 8874.764 . eval_reg(y, yhat) . Mean Absolute Error: 8870.10578571433 R2-Squared: 0.8280642147192246 . Predict new total case number . train_ca = series_to_supervised(values_ca, n_in=6) # split into input and output columns trainX, trainy = train_ca[:, :-1], train_ca[:, -1] # fit model model = RandomForestRegressor(n_estimators=10000) model.fit(trainX, trainy) # construct an input for a new prediction row = values_ca[-6:].flatten() . for i in range(15): row=row[-6:] print(&#39;Input: %s, Predicted: %.3f&#39; % (row, model.predict(asarray([row]))[0])) row = np.append(row, model.predict(asarray([row]))[0]) . Input: [4984465 4992875 4997390 5001302 5009101 5014428], Predicted: 5011117.356 Input: [4992875. 4997390. 5001302. 5009101. 5014428. 5011117.3563], Predicted: 5011117.356 Input: [4997390. 5001302. 5009101. 5014428. 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5001302. 5009101. 5014428. 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5009101. 5014428. 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5014428. 5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 Input: [5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563 5011117.3563], Predicted: 5011117.356 . Plot for the predicted and forcast values . yhat_plot = np.append(yhat, row) plt.plot(yhat_plot, label=&#39;Forecast&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f8cb63aac90&gt;] . The predicted total case for California in the next 7 days is 5011117.356. . Finding the nodes . Our random forest model has an issue of predicting the same values for every single day. The largest value of your threshold is not able to split the 2 values you are trying to predict, hence they will always end up in the same nodes, giving you the same prediction. . threshold = np.unique([j for i in model.estimators_ for j in i.tree_.threshold]) . np.sort(threshold)[-10:] . array([4990785. , 4990927.5, 4992883.5, 4995132.5, 4996783. , 4997088.5, 4999346. , 5000988. , 5003245.5, 5005201.5]) . Since the largest value of your threshold is not able to split the the values at the last node, it always end up in the same node, giving the same prediction. Same issue happens for the fllowing states as well. . Random forest model for Florida . series_fl = df_state_fl[[&#39;tot_cases&#39;]] values_fl = series_fl.values # transform the time series data into supervised learning data_fl = series_to_supervised(values_fl, n_in=6) # evaluate mae, y, yhat = walk_forward_validation(data_fl, 14) print(&#39;MAE: %.3f&#39; % mae) . &gt;expected=3659501.0, predicted=3657004.5 &gt;expected=3659772.0, predicted=3658661.7 &gt;expected=3661582.0, predicted=3659352.9 &gt;expected=3663096.0, predicted=3660693.4 &gt;expected=3665628.0, predicted=3662216.6 &gt;expected=3667139.0, predicted=3664390.0 &gt;expected=3668749.0, predicted=3666121.1 &gt;expected=3669896.0, predicted=3667800.6 &gt;expected=3670931.0, predicted=3669130.6 &gt;expected=3672616.0, predicted=3670245.6 &gt;expected=3674329.0, predicted=3671767.3 &gt;expected=3676337.0, predicted=3673451.7 &gt;expected=3677967.0, predicted=3675303.8 &gt;expected=3679548.0, predicted=3677013.9 [3657004.508, 3658661.713, 3659352.907, 3660693.363, 3662216.576, 3664389.957, 3666121.074, 3667800.607, 3669130.613, 3670245.586, 3671767.262, 3673451.682, 3675303.815, 3677013.917] MAE: 2424.101 . plt.plot(y, label=&#39;Expected&#39;) plt.plot(yhat, label=&#39;Predicted&#39;) plt.legend() plt.show() . Evaluate the model . eval_reg(y, yhat) . Mean Absolute Error: 8874.763785714216 R2-Squared: 0.8280133200162414 . data_fl = series_to_supervised(values_ca, n_in=6) # evaluate mae, y, yhat = walk_forward_validation(data_fl, 14) print(&#39;MAE: %.3f&#39; % mae) . &gt;expected=4937188.0, predicted=4932127.1 &gt;expected=4937188.0, predicted=4935484.0 &gt;expected=4956081.0, predicted=4936511.0 &gt;expected=4960026.0, predicted=4949279.8 &gt;expected=4963742.0, predicted=4955932.8 &gt;expected=4972469.0, predicted=4960723.9 &gt;expected=4972469.0, predicted=4968587.1 &gt;expected=4984465.0, predicted=4971022.6 &gt;expected=4984465.0, predicted=4979399.5 &gt;expected=4992875.0, predicted=4982290.9 &gt;expected=4997390.0, predicted=4989324.8 &gt;expected=5001302.0, predicted=4994470.2 &gt;expected=5009101.0, predicted=4998506.8 &gt;expected=5014428.0, predicted=5005227.6 [4932127.143, 4935484.013, 4936511.011, 4949279.809, 4955932.765, 4960723.899, 4968587.143, 4971022.585, 4979399.512, 4982290.913, 4989324.758, 4994470.2, 4998506.826, 5005227.626] MAE: 8878.628 . Predict on new date . train_fl = series_to_supervised(values_fl, n_in=6) # split into input and output columns trainX, trainy = train_fl[:, :-1], train_fl[:, -1] # fit model model = RandomForestRegressor(n_estimators=10000) model.fit(trainX, trainy) # construct an input for a new prediction row = values_fl[-6:].flatten() . for i in range(15): row=row[-6:] print(&#39;Input: %s, Predicted: %.3f&#39; % (row, model.predict(asarray([row]))[0])) row = np.append(row, model.predict(asarray([row]))[0]) . Input: [3670931 3672616 3674329 3676337 3677967 3679548], Predicted: 3678600.730 Input: [3672616. 3674329. 3676337. 3677967. 3679548. 3678600.7296], Predicted: 3678600.730 Input: [3674329. 3676337. 3677967. 3679548. 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3676337. 3677967. 3679548. 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3677967. 3679548. 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3679548. 3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 Input: [3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296 3678600.7296], Predicted: 3678600.730 . yhat_plot = np.append(yhat, row) plt.plot(yhat_plot, label=&#39;Forecast&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f8cb6945d50&gt;] . The predicted total case for Florida in the next 7 days is 3678600.730. . Random forest model for Texas . series_tx = df_state_tx[[&#39;tot_cases&#39;]] values_tx = series_tx.values # transform the time series data into supervised learning data_tx = series_to_supervised(values_tx, n_in=6) # evaluate mae, y, yhat = walk_forward_validation(data_tx, 14) print(&#39;MAE: %.3f&#39; % mae) . &gt;expected=4243143.0, predicted=4238085.7 &gt;expected=4244349.0, predicted=4241266.3 &gt;expected=4245949.0, predicted=4243265.5 &gt;expected=4251485.0, predicted=4244959.2 &gt;expected=4255280.0, predicted=4249102.4 &gt;expected=4259194.0, predicted=4253087.4 &gt;expected=4262152.0, predicted=4256857.3 &gt;expected=4266084.0, predicted=4260284.5 &gt;expected=4266698.0, predicted=4264123.5 &gt;expected=4268082.0, predicted=4265674.1 &gt;expected=4272442.0, predicted=4267140.3 &gt;expected=4277364.0, predicted=4270620.6 &gt;expected=4282152.0, predicted=4274952.3 &gt;expected=4286622.0, predicted=4279324.0 [4238085.664, 4241266.299, 4243265.505, 4244959.239, 4249102.421, 4253087.372, 4256857.272, 4260284.546, 4264123.461, 4265674.146, 4267140.339, 4270620.586, 4274952.26, 4279324.018] MAE: 5160.919 . Plot expected vs predicted . plt.plot(y, label=&#39;Expected&#39;) plt.plot(yhat, label=&#39;Predicted&#39;) plt.legend() plt.show() . Evaluate the model . eval_reg(y, yhat) . Mean Absolute Error: 5160.91942857146 R2-Squared: 0.8353895233136869 . data_tx = series_to_supervised(values_tx, n_in=6) mae, y, yhat = walk_forward_validation(data_tx, 14) print(&#39;MAE: %.3f&#39; % mae) . &gt;expected=4243143.0, predicted=4238008.0 &gt;expected=4244349.0, predicted=4241579.3 &gt;expected=4245949.0, predicted=4243233.3 &gt;expected=4251485.0, predicted=4244910.0 &gt;expected=4255280.0, predicted=4249094.0 &gt;expected=4259194.0, predicted=4252878.3 &gt;expected=4262152.0, predicted=4256916.7 &gt;expected=4266084.0, predicted=4260161.6 &gt;expected=4266698.0, predicted=4264198.5 &gt;expected=4268082.0, predicted=4265557.9 &gt;expected=4272442.0, predicted=4267288.7 &gt;expected=4277364.0, predicted=4270405.1 &gt;expected=4282152.0, predicted=4274849.0 &gt;expected=4286622.0, predicted=4279665.5 [4238007.984, 4241579.319, 4243233.28, 4244909.988, 4249094.043, 4252878.333, 4256916.651, 4260161.59, 4264198.539, 4265557.884, 4267288.719, 4270405.081, 4274848.962, 4279665.458] MAE: 5160.726 . Predict on new date . train_tx = series_to_supervised(values_tx, n_in=6) # split into input and output columns trainX, trainy = train_tx[:, :-1], train_tx[:, -1] # fit model model = RandomForestRegressor(n_estimators=10000) model.fit(trainX, trainy) # construct an input for a new prediction row = values_tx[-6:].flatten() . for i in range(15): row=row[-6:] print(&#39;Input: %s, Predicted: %.3f&#39; % (row, model.predict(asarray([row]))[0])) row = np.append(row, model.predict(asarray([row]))[0]) yhat_plot = np.append(yhat, row) plt.plot(yhat_plot, label=&#39;Forecast&#39;) . Input: [4266698 4268082 4272442 4277364 4282152 4286622], Predicted: 4284003.455 Input: [4268082. 4272442. 4277364. 4282152. 4286622. 4284003.4549], Predicted: 4284003.455 Input: [4272442. 4277364. 4282152. 4286622. 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4277364. 4282152. 4286622. 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4282152. 4286622. 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4286622. 4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 Input: [4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549 4284003.4549], Predicted: 4284003.455 . [&lt;matplotlib.lines.Line2D at 0x7f8cb643c510&gt;] . The predicted total case for Texas in the next 7 days is 4284003.455. . Although the R^2 values for the random forest for all three states have value higher than 0.8, the predicted value for the future 7 days does not seem reasonable. After printing out the values for our thresholds, we found that the value at the right node is unable to split the predicted value so they all ended up with the same value. .",
            "url": "https://sl679.github.io/Candice_Li_823_blog/2021/11/24/final_ridge-and-rf.html",
            "relUrl": "/2021/11/24/final_ridge-and-rf.html",
            "date": " • Nov 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "823 HW3 -Candice Li",
            "content": "import csv import datetime import numpy as np import pandas as pd import matplotlib.pyplot as plt from ipywidgets import widgets . Read in the datasets . df_death = pd.read_csv(&#39;data/malaria_deaths.csv&#39;) df_inc = pd.read_csv(&#39;data/malaria_inc.csv&#39;) df_death_age =pd.read_csv(&#39;data/malaria_deaths_age.csv&#39;) . Rename the columns . After renaming the columns, the column names of the dataset are more concise and easier to use. . df_death.rename(columns = {&quot;Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)&quot;: &quot;death_rate&quot;}, inplace = True) df_inc.rename(columns = {&quot;Incidence of malaria (per 1,000 population at risk) (per 1,000 population at risk)&quot;: &quot;death_inc&quot;}, inplace = True) . Import the packages going to be used . I choose to use plotly for graphing . import plotly.io as pio import plotly.express as px import plotly.offline as py . Plot 1 . for &#39;malaria_deaths.csv&#39; data . fig1 = px.bar(df_death, x=&#39;Year&#39;, y=&#39;death_rate&#39;, color=&#39;Entity&#39;, title=&#39;Histogram of number of death of People due to Malaria in various countries from 1990 to 2016&#39;) fig1.update_layout() . def create_scatter1(select_country): with plt.style.context(&quot;ggplot&quot;): fig = plt.figure(figsize=(8,4)) plt.scatter(x = df_death[df_death.Entity == select_country].Year, y = df_death[df_death.Entity == select_country].death_rate, s = 10 ) plt.xlabel(&quot;Year&quot;) plt.title(&quot;Death Rate of Malaria from 1990 to 2016 in %s&quot; % (select_country)) . widgets.interact(create_scatter1, select_country = df_death[&quot;Entity&quot;].unique()); . Plot 2 . for &#39;malaria_inc.csv&#39; data . def create_scatter2(select_country): with plt.style.context(&quot;ggplot&quot;): fig = plt.figure(figsize=(8,4)) plt.scatter(x = df_inc[df_inc.Entity == select_country].Year, y = df_inc[df_inc.Entity == select_country].death_inc, s = 10 ) plt.xlabel(&quot;Year&quot;) plt.ylabel(&quot;Incidence of malaria (per 1,000 population at risk)&quot;) plt.title(&quot;Death Rate of Malaria from 1990 to 2016 in %s&quot; % (select_country)) . widgets.interact(create_scatter2, select_country = df_inc[&quot;Entity&quot;].unique()); . Plot 3 . for &#39;malaria_deaths.csv&#39; data . def create_scatter3(select_country, select_age): with plt.style.context(&quot;ggplot&quot;): fig = plt.figure(figsize=(10,6)) selected_df = df_death_age.loc[(df_death_age.entity == select_country) &amp; (df_death_age.age_group == select_age)] plt.scatter(x = selected_df.year, y = selected_df.deaths, s = 10 ) plt.xlabel(&quot;Year&quot;) plt.ylabel(&quot;Number of Deaths&quot;) plt.title(&quot;Deaths of Malaria from 1990 to 2016 in %s for people %s&quot; % (select_country, select_age)) . widgets.interact(create_scatter3, select_country = df_death_age[&quot;entity&quot;].unique(), select_age = df_death_age[&quot;age_group&quot;].unique()); .",
            "url": "https://sl679.github.io/Candice_Li_823_blog/2021/11/23/HW3_Li_Candice.html",
            "relUrl": "/2021/11/23/HW3_Li_Candice.html",
            "date": " • Nov 23, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "823 HW1-Candice Li",
            "content": "Problem 6; Sum square difference; Solved by 496303 people . We define a function called sum_square_diff. First, we find the sum of squares of the of the numbers in a certain range. Then, we find the square of the sum of the numbers in the certain range. The we find the difference between the two results. We get 2640 for the difference. . def sum_square_diff(n): &quot;&quot;&quot;Get the difference between the sum of the squares of the first one hundred natural numbers and the square of the sum. n : integer The integer we want to conduct sum square difference on. &quot;&quot;&quot; sum_square = 0 square_sum = 0 for i in range(1, n+1): sum_square = sum_square + (i**2) for i in range(1, n+1): square_sum = square_sum + i square_sum = (square_sum ** 2) result = square_sum - sum_square return result print(sum_square_diff(10)) . 2640 . Problem 34; Digit factorials; Solved by 59089 people . We define a function called get_sum() to find the sum of all the curious numbers like 145. First we define a function to find the digits of a number called the get_digits. In this function, we convert the number to a string and put the digits in an array and then return the array. Then, we define the get_fuctorial function to get the factorial of a certain number. In this function, we use recursion. The base case is defined to be when the number is 1 or 0, else we multiply the number to the total result and minus 1 from the number and then pass it back to the function. After defining these functions, we find the number that the sum of the factorial of its digits equals itself and sum them up. For numbers between 3 and 9999999 because if all the digits are 9, the digits that possible fullfill the requirement of the curious number is around 7. For every number in this range, we pass every digits and get the sum of the factorial of them and compare the sum with the original number. If the two values are equal, then we add them into the result. Finally, we got 40730 for the sum. . def get_sum(): &quot;&quot;&quot;Get the sum of curious numbers which equals the sum of the factorial of its digits. &quot;&quot;&quot; def get_digits(number): &quot;&quot;&quot;Get the digints of a certain number number: integer The intger that we want to get digits of. &quot;&quot;&quot; digs = [] for i in str(number): digs.append(i) return digs def get_factorial(num): &quot;&quot;&quot;Get the factorial of a number num: integer The integer we want to get factorial of. &quot;&quot;&quot; if (num == 1 or num == 0): return 1 else: return num * get_factorial(num-1) total_sum = 0 for number in range(3, 1499999): digits = get_digits(number) fac_sum = 0 for dig in digits: fac_sum = fac_sum + get_factorial(int(dig)) if (fac_sum == number): total_sum = total_sum + number return(total_sum) print(get_sum()) . 40730 . Problem 112; Bouncy numbers; Solved by 24434 people. . We define a function called get_lim() to get the least number for which the proportion of bouncy numbers is exactly 99%. Under this function, first we call a function to find the digits of a number called the get_digits. In this function, we convert the number to a string and put the digits in an array and then return the array. Then, we define an is_increase() function to check if the number is an increasing number. In this function, we first call the get_digits() function to get digits of the number; then we compare the digits with the next digits to see if it is an increasing number. Then, we define an is_decrease() function to check if the number is a decreasing number. In this function, we first call the get_digits() function to get digits of the number; then we compare the digits with the next digits to see if it is a decreasing number. Thirdly, we define a is_bouncy() function to check if the number is a bouncy number. We call the is_decrease() and is_increase() function defined earlier. If the number is neither increasing or decreasing, then it is a bouncy number. Finally, we create a while loop to append every bouncy number in an array until the percent of bouncy numbers in the range get 99%. The result we got is 1587000. . def get_lim(): &quot;&quot;&quot;Find the least number for which the proportion of bouncy numbers is exactly 99% &quot;&quot;&quot; def get_digits(number): &quot;&quot;&quot; Get the dignits of integers. number: integer The integer we want to get digits of. &quot;&quot;&quot; digs = [] for i in str(number): digs.append(i) return digs def is_increase(number): &quot;&quot;&quot;Check if the integer is an increasing number number: integer The interger we want to check if is an increasing number. &quot;&quot;&quot; digits = get_digits(number) result = 0 for i in range(0,len(digits) - 1): if (int(digits[i+1]) &gt;= int(digits[i])): result = result + 0 else: result += 1 if (result == 0): return 1 else: return 0 return result def is_decrease(number): &quot;&quot;&quot;Check if the integer is a decreasing number number: integer The interger we want to check if is a decreasing number. &quot;&quot;&quot; digits = get_digits(number) result = 0 for i in range(0,len(digits) - 1): if (int(digits[i+1]) &lt;= int(digits[i])): result = result + 0 else: result += 1 if (result == 0): return 1 else: return 0 return result def is_bouncy(number): &quot;&quot;&quot;Check if the integer is a bouncy number number: integer The interger we want to check if is a bouncy number. &quot;&quot;&quot; if ((is_increase(number) != 1) &amp; (is_decrease(number) != 1)): return 1 else: return 0 number = 0; percent = 0; b =[] while percent &lt; 0.99: number += 1 if is_bouncy(number): b.append(number) percent = len(b)/number print(&quot;The least number for which the proportion of bouncy numbers is exactly 99% is &quot; + str(number)) get_lim() . The least number for which the proportion of bouncy numbers is exactly 99% is 1587000 .",
            "url": "https://sl679.github.io/Candice_Li_823_blog/2021/11/23/HW1_Li_Candice.html",
            "relUrl": "/2021/11/23/HW1_Li_Candice.html",
            "date": " • Nov 23, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "823 HW5 Candice Li",
            "content": "import the needed packages . import PIL from PIL import Image import glob import os import matplotlib.pyplot as plt import numpy as np import tensorflow as tf from tensorflow import keras from tensorflow.keras import layers from tensorflow.keras.models import Sequential . Define file path . path_train = &#39;train/&#39; path_test = &#39;test/&#39; . batch_size = 32 img_height = 180 img_width = 180 . Read training data and split into training and validation at ratio 0.2 . tr_ds = tf.keras.utils.image_dataset_from_directory( path_train, validation_split=0.2, subset=&quot;training&quot;, seed =123, image_size=(img_height, img_width), batch_size=batch_size) . Found 1019 files belonging to 3 classes. Using 816 files for training. . val_ds = tf.keras.utils.image_dataset_from_directory( path_train, validation_split=0.2, subset=&quot;validation&quot;, seed =123, image_size=(img_height, img_width), batch_size=batch_size) . Found 1019 files belonging to 3 classes. Using 203 files for validation. . Show some sample plots . plt.figure(figsize=(10, 10)) for images, labels in tr_ds.take(1): for i in range(9): ax = plt.subplot(3, 3, i + 1) plt.imshow(images[i].numpy().astype(&quot;uint8&quot;)) plt.title(class_names[labels[i]]) plt.axis(&quot;off&quot;) . for image_batch, labels_batch in tr_ds: print(image_batch.shape) print(labels_batch.shape) break . (32, 180, 180, 3) (32,) . Configure the dataset for performance . AUTOTUNE = tf.data.AUTOTUNE tr_ds = tr_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE) . Standardize the data . normalization_layer = layers.Rescaling(1./255) . normalized_ds = tr_ds.map(lambda x, y: (normalization_layer(x), y)) image_batch, labels_batch = next(iter(normalized_ds)) first_image = image_batch[0] # Notice the pixel values are now in `[0,1]`. print(np.min(first_image), np.max(first_image)) . 0.0 0.99623764 . Data augmentation . Generate more data from exisiting examples by augmenting them using random transformations . data_augmentation = keras.Sequential( [ layers.RandomFlip(&quot;horizontal&quot;, input_shape=(img_height, img_width, 3)), layers.RandomRotation(0.1), layers.RandomZoom(0.1), ] ) . Create the model using augmented data and use dropout to avoid overfitting . num_classes = 3 model = Sequential([ data_augmentation, layers.Rescaling(1./255), layers.Conv2D(16, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Conv2D(32, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Conv2D(64, 3, padding=&#39;same&#39;, activation=&#39;relu&#39;), layers.MaxPooling2D(), layers.Dropout(0.2), layers.Flatten(), layers.Dense(128, activation=&#39;relu&#39;), layers.Dense(num_classes) ]) . Compile the model . model.compile(optimizer=&#39;adam&#39;, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) . Model summary . model.summary() . Model: &#34;sequential_3&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= sequential_1 (Sequential) (None, 180, 180, 3) 0 rescaling_3 (Rescaling) (None, 180, 180, 3) 0 conv2d_6 (Conv2D) (None, 180, 180, 16) 448 max_pooling2d_6 (MaxPooling (None, 90, 90, 16) 0 2D) conv2d_7 (Conv2D) (None, 90, 90, 32) 4640 max_pooling2d_7 (MaxPooling (None, 45, 45, 32) 0 2D) conv2d_8 (Conv2D) (None, 45, 45, 64) 18496 max_pooling2d_8 (MaxPooling (None, 22, 22, 64) 0 2D) dropout_1 (Dropout) (None, 22, 22, 64) 0 flatten_2 (Flatten) (None, 30976) 0 dense_4 (Dense) (None, 128) 3965056 dense_5 (Dense) (None, 3) 387 ================================================================= Total params: 3,989,027 Trainable params: 3,989,027 Non-trainable params: 0 _________________________________________________________________ . Train the model . epochs = 15 history = model.fit( tr_ds, validation_data=val_ds, epochs=epochs ) . Epoch 1/15 26/26 [==============================] - 12s 432ms/step - loss: 1.0776 - accuracy: 0.5453 - val_loss: 0.7421 - val_accuracy: 0.6404 Epoch 2/15 26/26 [==============================] - 11s 430ms/step - loss: 0.6459 - accuracy: 0.7451 - val_loss: 0.4480 - val_accuracy: 0.8128 Epoch 3/15 26/26 [==============================] - 11s 427ms/step - loss: 0.5108 - accuracy: 0.8027 - val_loss: 0.3614 - val_accuracy: 0.8276 Epoch 4/15 26/26 [==============================] - 11s 430ms/step - loss: 0.4420 - accuracy: 0.8419 - val_loss: 0.4455 - val_accuracy: 0.8128 Epoch 5/15 26/26 [==============================] - 11s 427ms/step - loss: 0.4379 - accuracy: 0.8321 - val_loss: 0.4074 - val_accuracy: 0.8276 Epoch 6/15 26/26 [==============================] - 11s 431ms/step - loss: 0.3740 - accuracy: 0.8505 - val_loss: 0.3542 - val_accuracy: 0.8571 Epoch 7/15 26/26 [==============================] - 11s 424ms/step - loss: 0.3789 - accuracy: 0.8542 - val_loss: 0.4123 - val_accuracy: 0.8621 Epoch 8/15 26/26 [==============================] - 11s 429ms/step - loss: 0.3630 - accuracy: 0.8615 - val_loss: 0.3222 - val_accuracy: 0.8670 Epoch 9/15 26/26 [==============================] - 11s 424ms/step - loss: 0.3783 - accuracy: 0.8493 - val_loss: 0.3583 - val_accuracy: 0.8719 Epoch 10/15 26/26 [==============================] - 11s 427ms/step - loss: 0.3393 - accuracy: 0.8664 - val_loss: 0.3283 - val_accuracy: 0.8818 Epoch 11/15 26/26 [==============================] - 11s 425ms/step - loss: 0.3311 - accuracy: 0.8701 - val_loss: 0.2955 - val_accuracy: 0.8670 Epoch 12/15 26/26 [==============================] - 11s 442ms/step - loss: 0.3095 - accuracy: 0.8811 - val_loss: 0.4409 - val_accuracy: 0.8473 Epoch 13/15 26/26 [==============================] - 11s 425ms/step - loss: 0.2825 - accuracy: 0.8848 - val_loss: 0.3919 - val_accuracy: 0.8621 Epoch 14/15 26/26 [==============================] - 11s 433ms/step - loss: 0.2888 - accuracy: 0.8934 - val_loss: 0.3586 - val_accuracy: 0.8768 Epoch 15/15 26/26 [==============================] - 12s 472ms/step - loss: 0.2819 - accuracy: 0.8824 - val_loss: 0.3863 - val_accuracy: 0.8571 . Visualize training results . acc = history.history[&#39;accuracy&#39;] val_acc = history.history[&#39;val_accuracy&#39;] loss = history.history[&#39;loss&#39;] val_loss = history.history[&#39;val_loss&#39;] epochs_range = range(epochs) plt.figure(figsize=(8, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=&#39;Training Accuracy&#39;) plt.plot(epochs_range, val_acc, label=&#39;Validation Accuracy&#39;) plt.legend(loc=&#39;lower right&#39;) plt.title(&#39;Training and Validation Accuracy&#39;) plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=&#39;Training Loss&#39;) plt.plot(epochs_range, val_loss, label=&#39;Validation Loss&#39;) plt.legend(loc=&#39;upper right&#39;) plt.title(&#39;Training and Validation Loss&#39;) plt.show() . x_train = np.concatenate([x for x, y in tr_ds], axis = 0) y_train = np.concatenate([y for x, y in tr_ds], axis = 0) x_test = np.concatenate([x for x, y in te_ds], axis = 0) y_test = np.concatenate([y for x, y in te_ds], axis = 0) . Evaluate the model using the test data using evaluate . print(&quot;Evaluate on test data&quot;) results = model.evaluate(x_test, y_test, batch_size=128) print(&quot;test loss, test acc:&quot;, results) . Evaluate on test data 2/2 [==============================] - 1s 170ms/step - loss: 0.2914 - accuracy: 0.8833 test loss, test acc: [0.29136383533477783, 0.8833333253860474] . p_path = &#39;purple_drangonfly.jpg&#39; img_p = PIL.Image.open(p_path) plt.imshow(img_p) plt.show() img = tf.keras.utils.load_img( p_path, target_size=(img_height, img_width) ) img_array = tf.keras.utils.img_to_array(img) img_array = tf.expand_dims(img_array, 0) # Create a batch predictions = model.predict(img_array) score = tf.nn.softmax(predictions[0]) print( &quot;This image most likely belongs to {} with a {:.2f} percent confidence.&quot; .format(class_names[np.argmax(score)], 100 * np.max(score)) ) . This image most likely belongs to dragonflies with a 97.52 percent confidence. . Use shapley additive explanations to explain the output of model . explainer = shap.GradientExplainer(model, x_train) . sv = explainer.shap_values(x_test[:10]) . `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model. . shap.image_plot([sv[i] for i in range(3)], x_test[85:88]) .",
            "url": "https://sl679.github.io/Candice_Li_823_blog/2021/11/14/823HW5_Li_Candice.html",
            "relUrl": "/2021/11/14/823HW5_Li_Candice.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "823 HW2 -Candice Li",
            "content": "In order to find the first 10-digit prime in the decimal expansion of 17π, three functions were defined. . Import libraries going to be used . import math import decimal try: from sympy.mpmath import mp except ImportError: from mpmath import mp . The function expansion_generater is used to get n places after the decimal point for irrational number. First the decimal places after the decimal point is defined to be 1000 so that it is long enough to find the target segment. Then, the actual expression being used is genrated based on the input parameters. In this program, only e and π are being considered. Then, the part after the decimal point of the expression with desired length is being extracted and returned. . def expansion_generater(n, symbol, time): &quot;&quot;&quot;Get n places after the decimal point for irrational number. n : int The length of the wanted number after decimal places. &quot;&quot;&quot; mp.dps = 1000 if symbol == &#39;pi&#39;: a = mp.pi elif symbol == &#39;e&#39;: a = mp.e exp = a*time before_dec, after_dec = int(exp), exp - int(exp) return str(after_dec)[2:n+2] . The function is_prime is used to check if an integer is a prime number. First, special cases like 0, 1, and 2 are being accounted for. 0, 1 are hard-coded to be not prime and 2 is hard-coded to be prime. Secondly, all the even numbers are coded to be not prime. Then we check if the square root of the number can be diviide by any number smaller than itself. If there is one, then this number is not prime. Otherwise, it is a prime number. . def is_prime(n): &quot;&quot;&quot;Check if an integer is a prime number. n : int The integer we want to check if it is prime. &quot;&quot;&quot; if n == 0 or n == 1: return False elif n == 2: return True elif (n &gt; 2 and n % 2 == 0): return False else: for i in range (3, 1 + math.floor(math.sqrt(n)), 2): if n % i == 0: return False return True . The function get_window is used to get all the integers with desired length in an expression. First, the expression is converted from integer to string. Secondly, a list is created to store the integers. Then, we start from the first index take out every integer segment with desired length. . def get_window(n, expression): &quot;&quot;&quot;Get the integers of length n in an expression. n : int The length of the wanted intger in the expression. &quot;&quot;&quot; expression = str(expression) result = [] for i in range(0, len(expression)): if len(expression[i:i+n]) == n: result.append(expression[i:i+n]) return result . The function find_n_digit_prime is the function that calls all of the functions written above and return the first value that satisfy the requirement. First, we get the segments of desried length using the function get_window and expansion_generator. expansion_generator returns the expression based on input and get_window get the segments with certain lenth in the expression. The, we store the segments if it has the same length with the desired length (the length may be shorter when reaching the end of the expression). Fianlly, we return the first segment in this list. This interger will be the final answer from this program. . def find_n_digit_prime(total_len, symbol, time, seg_len): segments = get_window(seg_len, expansion_generater(total_len, symbol, time)) found = [] for j in range (0, len(segments)): if is_prime(int(segments[j])): found.append(int(segments[j])) return found[0] . Run the code and get the answer for the first 10-digit prime in the decimal expansion of 17&#960;. . answer = find_n_digit_prime(1000, &quot;pi&quot;, 17, 10) print(&quot;The first 10-digit prime in the decimal expansion of 17π is &quot;+ str(answer) +&quot;.&quot;) . The first 10-digit prime in the decimal expansion of 17π is 8649375157. . Unit tests . Test for funtion expansion_generater . import unittest . class TestNotebook(unittest.TestCase): def expansion_generater(self): self.assertEqual(expansion_generater(10, &#39;pi&#39;, 1), 1415926535) self.assertEqual(expansion_generater(11, &#39;2&#39;, 2), 43656365692) unittest.main(argv = [&#39;&#39;], verbosity = 2, exit = False) . - Ran 0 tests in 0.000s OK . &lt;unittest.main.TestProgram at 0x7fe6ee1b1210&gt; . Test for funtion expansion_generater . class TestNotebook(unittest.TestCase): def is_prime(self): self.assertEqual(is_prime(150), False) self.assertEqual(is_prime(11), True) self.assertEqual(is_prime(2), True) unittest.main(argv = [&#39;&#39;], verbosity = 2, exit = False) . - Ran 0 tests in 0.000s OK . &lt;unittest.main.TestProgram at 0x7fe6ee24f210&gt; . Test for funtion get_window . class TestNotebook(unittest.TestCase): def get_window(self): self.assertEqual(get_window(1, 12345), [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;]) self.assertEqual(get_window(2, 12345), [&#39;12&#39;, &#39;23&#39;, &#39;34&#39;, &#39;45&#39;]) self.assertEqual(get_window(3, 12345), [&#39;123&#39;, &#39;234&#39;, &#39;345&#39;]) unittest.main(argv = [&#39;&#39;], verbosity = 2, exit = False) . - Ran 0 tests in 0.000s OK . &lt;unittest.main.TestProgram at 0x7fe6ee1d4050&gt; . Test for funtion find_first_ten_digit_prime . class TestNotebook(unittest.TestCase): def test_n_ten_digit_prime(self): self.assertEqual(find_first_ten_digit_prime(1000, &#39;e&#39;, 1, 10), 7427466391) unittest.main(argv = [&#39;&#39;], verbosity = 2, exit = False) . test_n_ten_digit_prime (__main__.TestNotebook) ... ok - Ran 1 test in 0.175s OK . &lt;unittest.main.TestProgram at 0x7fe6ee223390&gt; .",
            "url": "https://sl679.github.io/Candice_Li_823_blog/2021/09/17/823HW2_Li_Candice.html",
            "relUrl": "/2021/09/17/823HW2_Li_Candice.html",
            "date": " • Sep 17, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://sl679.github.io/Candice_Li_823_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://sl679.github.io/Candice_Li_823_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://sl679.github.io/Candice_Li_823_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://sl679.github.io/Candice_Li_823_blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}